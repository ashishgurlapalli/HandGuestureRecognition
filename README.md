Hand Gesture Recognition System
A comprehensive real-time hand gesture recognition system that uses computer vision and machine learning to detect, classify, and recognize custom hand gestures through a webcam interface.
ğŸŒŸ Features
Custom Gesture Training: Collect and label unlimited hand gestures

Real-time Recognition: 30+ FPS performance with live webcam feed

Web-based Interface: Beautiful Streamlit frontend for easy interaction

Machine Learning: SVM-based classification with 85-95% accuracy

Data Management: Easy gesture management and deletion tools

Cross-Platform: Works on Windows, macOS, and Linux

ğŸ—ï¸ System Architecture:

HandGuesture/
â”œâ”€â”€ ğŸ“Š app.py                 # Streamlit web application
â”œâ”€â”€ ğŸ¥ collect_data.py        # Gesture data collection module
â”œâ”€â”€ ğŸ¤– train_model.py         # Machine learning model training
â”œâ”€â”€ ğŸ” recognize.py           # Real-time gesture recognition
â”œâ”€â”€ ğŸ› ï¸ utils.py              # Helper functions and utilities
â”œâ”€â”€ ğŸ“ gesture_manager.py     # Gesture data management
â”œâ”€â”€ ğŸ“‹ requirements.txt       # Python dependencies
â”œâ”€â”€ ğŸ“ README.md             # Project documentation
â”œâ”€â”€ ğŸ“‚ data/                 # Gesture sample storage
â”‚   â””â”€â”€ ğŸ“‚ gesture_name/
â”‚       â””â”€â”€ ğŸ“„ sample_0.npy  # Landmark data files
â””â”€â”€ ğŸ“‚ model/                # Trained models
    â”œâ”€â”€ ğŸ“„ gesture_model.pkl # SVM classifier
    â””â”€â”€ ğŸ“„ label_encoder.pkl # Label encoder

ğŸš€ Quick Start
Prerequisites
1.Python 3.8+
2.Webcam
3.4GB+ RAM

Installation:
1.Clone the repository
-git clone https://github.com/your-username/hand-gesture-recognition.git
-cd hand-gesture-recognition

2.Create virtual environment
-python -m venv .venv
-source .venv/bin/activate  # Linux/macOS
-.venv\Scripts\activate     # Windows

3.Install dependencies
-pip install -r requirements.txt

4.Launch the application
-streamlit run app.py

ğŸ“– Usage Guide
1. Data Collection
Navigate to "Collect Data" tab
Enter a gesture name (e.g., "thumbs_up")
Click "Start Data Collection"
Show your hand to the camera
Press 's' to save samples (20-30 recommended)
Press 'q' when finished

2. Model Training
Collect at least 2 different gestures
Go to "Train Model" tab
Click "Start Training"
Wait for training to complete (1-5 minutes)
Review accuracy metrics

3. Real-time Recognition
Ensure model is trained
Navigate to "Real-time Recognition" tab
Click "Start Recognition"
Perform gestures in front of camera
View real-time predictions
Press 'q' to exit recognition window

ğŸ› ï¸ Technical Details

Computer Vision Pipeline
Hand Detection: MediaPipe Hands for 21-point landmark detection
Landmark Extraction: 63 features (21 landmarks Ã— 3 coordinates)
Real-time Processing: 30+ FPS on standard hardware

Machine Learning
Algorithm: Support Vector Machine (SVM) with RBF kernel
Features: 63-dimensional landmark vectors
Training: 80-20 train-test split with stratification
Accuracy: 85-95% with sufficient training data

Data Format:
Each gesture sample is stored as a 63-element NumPy array:
[x0, y0, z0, x1, y1, z1, ..., x20, y20, z20]  # 21 landmarks Ã— 3 coordinates


ğŸ“Š Performance Metrics
Frame Rate: 30+ FPS (640Ã—480 resolution)
Accuracy: 85-95% (with 20+ samples per gesture)
Latency: <100ms end-to-end recognition
Memory Usage: ~500MB RAM



